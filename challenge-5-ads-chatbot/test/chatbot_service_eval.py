"""
Evaluation script for the Alaska Department of Snow chatbot responses.

This script uses Google's Evaluation API to assess the quality of responses
generated by the chatbot based on various metrics like groundedness,
verbosity, instruction following, and safety.
"""

import sys
import os
from datetime import datetime
import pandas as pd

# Add the src directory to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.chatbot_service import call_agent
from vertexai.evaluation import EvalTask, MetricPromptTemplateExamples

# Sample queries for evaluation
test_queries = [
    "What is the mission of Alaska Department of Snow?",
    "How do I report an unplowed road?",
    "What are the snow removal priorities after a storm?",
    "When will my residential street be plowed?",
    "What should I do if my car gets stuck in snow?",
    "How does the department decide which roads to plow first?",
    "What time do snow plows typically start working?",
    "How can I find out if my street has been plowed?",
    "What's the budget for snow removal in Alaska?"
]

def create_evaluation_dataset():
    """
    Create an evaluation dataset with sample queries and responses.
    
    Returns:
        pandas.DataFrame: A dataframe with prompt and response columns
    """
    print("Generating responses for evaluation dataset...")
    
    # Generate responses for each query
    responses = []
    for query in test_queries:
        print(f"Processing query: {query}")
        response = call_agent(query)
        responses.append(response)
        print(f"Response: {response[:100]}...\n")
    
    # Create a dataframe with prompts and responses
    eval_data = pd.DataFrame({
        "prompt": test_queries,
        "response": responses
    })
    
    return eval_data

def run_evaluation():
    """
    Run evaluation on the chatbot responses using Google's Evaluation API.
    
    This function:
    1. Creates an evaluation dataset
    2. Sets up an EvalTask with appropriate metrics
    3. Runs the evaluation
    4. Displays and returns the results
    
    Returns:
        The evaluation results
    """
    # Create evaluation dataset
    eval_dataset = create_evaluation_dataset()
    
    # Generate a timestamp for the experiment
    ts = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # Set up the evaluation task
    eval_task = EvalTask(
        dataset=eval_dataset,
        metrics=[
            MetricPromptTemplateExamples.Pointwise.GROUNDEDNESS,
            MetricPromptTemplateExamples.Pointwise.VERBOSITY,
            MetricPromptTemplateExamples.Pointwise.INSTRUCTION_FOLLOWING,
            MetricPromptTemplateExamples.Pointwise.SAFETY
        ],
        experiment=f"alaska-snow-chatbot-{ts}"
    )
    
    print("\nRunning evaluation...")
    
    # Run the evaluation
    result = eval_task.evaluate(
        prompt_template="Query: {prompt}. Response: {response}",
        experiment_run_name=f"alaska-snow-chatbot-{ts}"
    )
    
    print("\nEvaluation complete!")
    
    # Display the results
    print("\nEvaluation Results:")
    print("==================")
    
    # Display overall metrics
    print("\nOverall Metrics:")
    for metric_name, metric_value in result.summary_metrics.items():
        print(f"{metric_name}: {metric_value}")
    
    # Display the metrics table
    print("\nDetailed Results (Metrics Table):")
    pd.set_option('display.max_columns', None)  # Show all columns
    pd.set_option('display.width', 1000)        # Wide display
    pd.set_option('display.max_colwidth', 50)   # Limit column width for readability
    print(result.metrics_table)
    
    # Save the metrics table to a CSV file
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    csv_filename = f"evaluation_results_{timestamp}.csv"
    result.metrics_table.to_csv(csv_filename, index=False)
    print(f"\nDetailed results saved to {csv_filename}")
    
    return result

if __name__ == "__main__":
    print("Starting evaluation of Alaska Department of Snow chatbot...")
    eval_results = run_evaluation()
    print("\nEvaluation script completed.")
