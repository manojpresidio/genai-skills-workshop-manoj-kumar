from src.prompts import system_prompt, input_safety_filter_prompt, response_safety_filter_prompt
from src.utils import extract_json
from src.gemini_client import call_llm
from src.dao import bq_vector_search

def input_safety_check(user_input):
    """
    Check if the user input is safe to process by evaluating it against safety guidelines.
    
    This function uses the input safety filter prompt to determine if the user's query
    is appropriate for the Alaska Department of Snow chatbot. It filters out:
    - Off-topic queries not related to snow management or Alaska
    - Attempts to jailbreak the AI or make it ignore its guidelines
    - Requests for harmful, offensive, or inappropriate content
    
    Args:
        user_input (str): The input text from the user
        
    Returns:
        dict: A dictionary with the following keys:
            - "decision": Either "safe" or "unsafe"
            - "reasoning": Explanation for why the input was classified as safe or unsafe
    """
    # Get the safety filter prompt
    prompt = input_safety_filter_prompt()
    
    # Call LLM with the safety filter prompt and user input
    response = call_llm(
        prompt=user_input,
        system_prompt=prompt,
        temperature=0.1  # Lower temperature for more deterministic safety checks
    )
    
    # Extract JSON from the response
    result = extract_json(response)
    
    # Return the result or a default unsafe response if extraction failed
    return result or {"decision": "unsafe", "reasoning": "Failed to parse safety check response"}

def output_safety_check(llm_output):
    """
    Check if the LLM-generated output is safe and appropriate to show to the user.
    
    This function uses the response safety filter prompt to evaluate if the generated
    response adheres to the Alaska Department of Snow chatbot guidelines. It filters out:
    - Responses containing harmful, offensive, or inappropriate content
    - Responses that deviate from the Alaska Department of Snow's domain
    - Responses that indicate the AI has been jailbroken or is ignoring guidelines
    - Responses with incorrect or misleading information
    
    Args:
        llm_output (str): The response generated by the LLM
        
    Returns:
        dict: A dictionary with the following keys:
            - "decision": Either "safe" or "unsafe"
            - "reasoning": Explanation for why the output was classified as safe or unsafe
    """
    # Get the safety filter prompt
    prompt = response_safety_filter_prompt()
    
    # Call LLM with the safety filter prompt and the generated output
    response = call_llm(
        prompt=llm_output,
        system_prompt=prompt,
        temperature=0.1  # Lower temperature for more deterministic safety checks
    )
    
    # Extract JSON from the response
    result = extract_json(response)
    
    # Return the result or a default unsafe response if extraction failed
    return result or {"decision": "unsafe", "reasoning": "Failed to parse safety check response"}

def process_user_input(user_input, history=None):
    """
    Process user input and generate a response using the LLM with context from the knowledge base.
    
    This function implements the RAG (Retrieval-Augmented Generation) pattern by:
    1. Retrieving relevant context from BigQuery using vector search
    2. Creating an augmented prompt that combines the retrieved context with the user's question
    3. Generating a response using the LLM with the system prompt and augmented prompt
    
    Args:
        user_input (str): The input text from the user
        history (list, optional): Conversation history as a list of message dictionaries
        
    Returns:
        str: The generated response from the LLM
    """
    # Get the system prompt
    system_instructions = system_prompt()
    
    # Retrieve relevant context from BigQuery
    context = bq_vector_search(user_input)
    
    # Create a prompt that includes the context and the user's question
    augmented_prompt = f"""
    Based on the following information from the Alaska Department of Snow knowledge base:
    
    {context}
    
    Please answer the user's question: {user_input}
    
    If the knowledge base doesn't contain information directly relevant to the question, 
    use your general knowledge about snow management and road conditions in Alaska, but 
    make it clear that you're providing general information rather than specific 
    department policies or data.
    """
    
    # Call the LLM with the augmented prompt and system prompt
    response = call_llm(
        prompt=augmented_prompt,
        system_prompt=system_instructions,
        temperature=0.3,  # Slightly higher temperature for more natural responses
        history=history
    )
    
    return response

def call_agent(user_input):
    """
    Main entry point for processing user queries with safety checks and RAG-based response generation.
    
    This function implements the complete chatbot pipeline:
    1. Checks if the user input is safe using input_safety_check
    2. If safe, retrieves context from the knowledge base and generates a response using process_user_input
    3. Checks if the generated response is safe using output_safety_check
    4. Returns either the safe response or an appropriate fallback message
    
    The function ensures that all responses are:
    - Relevant to the Alaska Department of Snow
    - Safe and appropriate for users
    - Grounded in the knowledge base when possible
    
    Args:
        user_input (str): The input text from the user
        
    Returns:
        str: A response to the user's query, or a polite refusal if the input or output is unsafe
    """
    # Check if the user input is safe
    safety_check = input_safety_check(user_input)
    
    # If the input is unsafe, return a polite refusal
    if safety_check.get("decision") == "unsafe":
        return f"I'm sorry, but I can only assist with questions related to the Alaska Department of Snow. {safety_check.get('reasoning', '')}"
    
    # Process the user input to generate a response
    response = process_user_input(user_input)
    
    # Check if the generated response is safe
    output_check = output_safety_check(response)
    
    # If the output is unsafe, return a fallback message
    if output_check.get("decision") == "unsafe":
        return "I apologize, but I'm unable to provide a response to that query. Please try asking something related to snow management or road conditions in Alaska."
    
    # Return the safe response
    return response
