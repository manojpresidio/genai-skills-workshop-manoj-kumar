{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG system that uses BigQuery to generate embeddings and perform a vector search."
      ],
      "metadata": {
        "id": "KxCOs0pudX5g"
      },
      "id": "KxCOs0pudX5g"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all the related libraries\n",
        "from google.cloud import bigquery\n",
        "from google import genai\n",
        "from google.genai import types"
      ],
      "metadata": {
        "id": "iJ_a2x0Iv_Wm"
      },
      "id": "iJ_a2x0Iv_Wm",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Constants ----------------\n",
        "PROJECT_ID = \"qwiklabs-gcp-02-682b5eee4362\"\n",
        "\n",
        "# Latest stable Gemini model version\n",
        "\n",
        "MODEL = \"gemini-2.0-flash-001\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant specialized in answering questions related to Aurora Bay, Alaska.\n",
        "Only respond based on the provided context. If the answer is not present in the context, reply with:\n",
        "\"I’m sorry, I don’t have enough information to answer that.\"\n",
        "IMPORTANT: Do not use any external knowledge.\n",
        "\"\"\"\n",
        "\n",
        "searchable_query_prompt = \"\"\"\n",
        "You are a helpful assistant. Your task is to rewrite the user's natural language question into a concise and meaningful query suitable for vector search in BigQuery.\n",
        "Focus on the core intent of the question. Remove unnecessary words, greetings, or filler phrases. Keep the output short, clear, and focused on keywords that best represent the user's information need without adding any additional content.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rayIjx2EpID3"
      },
      "id": "rayIjx2EpID3",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Big query client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Initialize the Gemini AI client to interact with the Vertex AI service.\n",
        "# Parameters:\n",
        "# - vertexai=True : specifies that the client should use Google Cloud Vertex AI as the backend.\n",
        "# - project : GCP project ID where the Vertex AI resources are located.\n",
        "# - location : Regional endpoint to connect to; \"global\" refers to a global endpoint.\n",
        "gemini_client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=PROJECT_ID,\n",
        "    location=\"global\",\n",
        ")"
      ],
      "metadata": {
        "id": "NJAFjMHZw8tI"
      },
      "id": "NJAFjMHZw8tI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Vector search on Bigquery\n",
        "def bq_vector_search(user_query):\n",
        "  \"\"\"\n",
        "  Runs the vector search on bigquery and retrieves the context for LLM\n",
        "  Parameter:\n",
        "    user_query: Query provided by the user\n",
        "  Returns:\n",
        "    Context to be sent to the LLM to generate response\n",
        "  \"\"\"\n",
        "  query = f\"\"\"\n",
        "  SELECT\n",
        "    query.query,\n",
        "    base.content as question,\n",
        "    base.answer\n",
        "  FROM\n",
        "      VECTOR_SEARCH(\n",
        "          TABLE `AuroraBayFAQ.FAQ_embedded`,\n",
        "          'ml_generate_embedding_result',\n",
        "          (\n",
        "              SELECT\n",
        "                  ml_generate_embedding_result,\n",
        "                  content AS query\n",
        "              FROM\n",
        "                  ML.GENERATE_EMBEDDING(\n",
        "                      MODEL `AuroraBayFAQ.Embeddings`,\n",
        "                      (SELECT '{user_query.strip()}' AS content)\n",
        "                  )\n",
        "          ),\n",
        "          top_k => 5,\n",
        "          options => '{{\"fraction_lists_to_search\": 0.01}}'\n",
        "      );\n",
        "  \"\"\"\n",
        "\n",
        "  job = bq_client.query(query)\n",
        "  results = job.result()\n",
        "\n",
        "  # Collect the results\n",
        "  context_chunks = []\n",
        "  for row in results:\n",
        "      context_chunks.append(f\"Question: {row.question}\\nAnswer: {row.answer}\")\n",
        "\n",
        "  # Combine context for LLM input\n",
        "  context = \"\\n\\n\".join(context_chunks)\n",
        "  return context"
      ],
      "metadata": {
        "id": "j1chht0swjFL"
      },
      "id": "j1chht0swjFL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_searchable_query(user_input):\n",
        "  \"\"\"\n",
        "  Generate a searchable query for Big query from User input\n",
        "  Returns:\n",
        "    Searchable query for Big query without any greeting, unnecessary fillers etc.,\n",
        "  \"\"\"\n",
        "  response = gemini_client.models.generate_content(\n",
        "    model = MODEL,\n",
        "    contents = [f\"Input Prompt: ${user_input}\"],\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        system_instruction=[types.Part.from_text(text=searchable_query_prompt)]\n",
        "    )\n",
        "  )\n",
        "  return response.text\n"
      ],
      "metadata": {
        "id": "rW2cCqkqzVIi"
      },
      "id": "rW2cCqkqzVIi",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Infinite loop to simulate the chat\n",
        "while True:\n",
        "  user_input = input(\"You: \")\n",
        "  if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "      print(\"Thank You!\")\n",
        "      break\n",
        "\n",
        "  # Generate a searchable query from user's natural language query\n",
        "  searchable_query = generate_searchable_query(user_input)\n",
        "\n",
        "  # Get context from big query using user's query\n",
        "  context = bq_vector_search(searchable_query)\n",
        "\n",
        "  # Prompt with context to generate response to the user's query\n",
        "  input_prompt = f\"\"\"\n",
        "  Use the following context to answer the given question.\n",
        "  Context: {context}\n",
        "  Question: {user_input}\n",
        "  \"\"\"\n",
        "\n",
        "  # Call LLM to get the response\n",
        "  response = gemini_client.models.generate_content(\n",
        "      model = MODEL,\n",
        "      contents = [f\"Input Prompt: ${input_prompt}\"],\n",
        "      config = types.GenerateContentConfig(\n",
        "          temperature = 0,\n",
        "          top_p = 1,\n",
        "          system_instruction=[types.Part.from_text(text=system_prompt)]\n",
        "      )\n",
        "    )\n",
        "  print(f\"Gemini: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw0NYKmgCxPu",
        "outputId": "6dff655e-8f5d-4cbe-e437-6ec01da6c989"
      },
      "id": "Vw0NYKmgCxPu",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: How is the weather out there in San Francisco Bay?\n",
            "Gemini: I’m sorry, I don’t have enough information to answer that.\n",
            "You: How is the weather out there in Aurora bay?\n",
            "Gemini: Winters average between 10°F to 25°F, while summers are milder, around 50°F to 65°F. Temperatures can vary with coastal weather patterns.\n",
            "\n",
            "You: How to reach Aurora bay?\n",
            "Gemini: Most visitors arrive via regional flights into Aurora Bay Airport or by ferry from nearby coastal towns. Small cruise ships also make seasonal stops.\n",
            "\n",
            "You: Is there any library available in Aurora bay?\n",
            "Gemini: Yes. The Aurora Bay Public Library is located on Main Street, next to the town’s post office.\n",
            "\n",
            "You: Exit\n",
            "Thank You!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-00-76e379869e75 (Jun 16, 2025, 12:03:24 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}